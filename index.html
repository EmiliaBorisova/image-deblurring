<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="style.css" rel="stylesheet">
    <script src="index.js"></script>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Assistant:wght@400&family=Bebas+Neue&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=PT+Serif:wght@700&display=swap" rel="stylesheet">
    <title>Image Deblurring</title>
</head>
<body>
    <header>
      <h1>Image Deblurring</h1>
      <blockquote>
        Created by Emilia Borisova, Fatimah Shabbir, Jack Seery, Thevina Dokka, Ege Caglar 
      </blockquote>
    </header>
    <main>
      <section class="box">
        <h2> Problem setup </h2>
        <p>
            The problem we are trying to solve is deblurring images, or making them sharper and more clear. We started with an image dataset of blurred images and then applied our deep learning models to try to bring the blurred images back to their original clarity.        
        </p>
        <p>
            Image deblurring is crucial to recover images that were taken hurriedly or without enough time to focus the camera, typically caused by shaky hands (or shaky camera) or taken while in motion. Blurry images can decrease the quality of the image and cover crucial details captured in the image. By using deep learning to clear images, it can bring more information to the image such as helping with facial recognition, text clarity, image detection, and many more things. Deblurring images are especially useful to law enforcement to help interpret blurry photo evidence, or even just to the average photographer to improve the quality and clarity of their photos that may have been taken in a rush.
        </p>
        <p>
            This problem of deblurring images has been less popular among deep learning research, but recently many research papers have been published that propose various methods for addressing the issue (Aittala & Durand, 2018; Albluwi et al., 2018; Rath, 2020). We used many of these papers for inspiration on constructing our own neural networks to solve this issue and produce the most optimal deblurring models that we could.
        </p>
        <h2>[VIDEO HERE]</h2>  <!-- can just make a YouTube video and include it in an <iframe> here -->
      </section>
      <section class="box">
        <h2> Data used </h2>
        <p>
            The dataset we used was the ‘Blur Dataset’ from Kaggle (Alekseev, 2019). This dataset contains image triplets which have images in 3 different ways: defocused blurred, motion-blurred, and sharp images. As an additional comparison metric, we also generated images blurred using the Gaussian algorithm to have more blurry images that we can run our models on. After training our models, we compared the resulting image with the corresponding sharp image from the dataset to evaluate the effectiveness of our algorithms.         
        </p>
      </section>
      <section class="box">
        <h2> Process </h2>
        <h3>Training </h3>
        <p>
          We used a Google colab workspace for a lot of our model development and basic code testing, and we used a local Jupyter Lab notebook for our more intensive training. 
        </p>
        <p>
          The local server was set up by a team member with access to powerful hardware necessary for training. 
          The server was set up for remote access by all team members and featured the same collaborative workspace as the Google colab workspace.
        </p>
        <p>
          The local server used more powerful hardware than that available to us on the Google colab workspace, did not time out after any period of inactivity, and never artificially limited our code runtime, making it the ideal choice for our training.
        </p>
      <h3>
          Phase 1: Training on defocused blur, motion blur, and gaussian blurred images       
      </h3>
      <p> First, we set up a SCRNN model and implemented the PSNR and LPIPS evaluation metrics as described in the Techniques section. </p> 
      <p>
        After viewing our PSNR and LPIPS results, we picked out the top values and highlighted them in red on the Excel sheet linked below. Coincidentally, the best PSNR values (higher is better) and the best LPIPS values (lower is better) were from the same graphs. They also highlight which hyperparameters were best performing. Specifically, it seemed that all of the best values came from the Gaussian data set. This inspired us to dive deeper into Gaussian, so we artificially created 3 more data sets with various blur intensities – sigma values of 5, 10, and 25. This was Phase 2.
      </p>
      <h3>
          Phase 2: Training on gaussian sigma values of 5, 10, and 25       
      </h3>
      <p>
        Using the same model and both of the evaluation metrics, we found that the best results were spread across all of the various Gaussian blur values, but the very best seemed to be the graph with blur sigma value of 5, Relu activation, 3 layers, 0.01 learning rate, and batch size of 16. Both the PSNR and LPIPS values were the best for this model. Additionally, the better values were centered around a Gaussian blur of 5 and 10. Blur of 25 did not do very well. This makes sense since a larger blur amount might be harder to deblur than smaller blur values.         
      </p>
      </section>
      <section class="box">
        <h2> Techniques </h2>
        <h3>
            SRCNN Setup:
        </h3>
        <p>
          SCRNN is a network architecture and stands for Super Resolution Convolutional Neural Network (Albluwi, Krylov, & Dahyot, 2018). This is a deep learning technique with which we did a hyperparameter search to find which parameters worked best on which data sets. The loss function defined was MSE and the optimizer defined was Adam. We also implemented a learning rate scheduler such that if the loss value does not improve after 5 epochs, then the new learning rate will be the old one * 0.5. These are the parameters we used for each of the 3 blurred datasets (defocused, gaussian, and motion):
          <ul>
            <li>
              Batch Size: 16, 32, 64, 128
            </li>
            <li>
              Learning Rate: 0, 0.01, 0.0001, 0.000001
            </li>
            <li>
              Activation: Relu, tanh
            </li>
            <li>
              Layers: 3, 4, 5
            </li>
          </ul>      
        </p>
        <p>
          For each implementation, we decided to test each hyperparameter by itself, so we kept a constant learning rate, activation, and layer amount when we tested batch size and then kept a constant batch size, activation, and layer amount when we testing learning rate, and so on. This would have yielded 3 (datasets) x (4 (batch sizes) + 4 (learning rates) + 2 (activation functions) + 3 (layers)) = 39 models. However, since there would have been overlap and a repeating model with parameters batch size of 16, learning rate of 0.01, activation of Relu, and 3 layers, we omitted creating some models and the resulting amount was 30 total. 
        </p>
        <p>The parameters to be tested in Phase 2 were decided based on the better performing models in Phase 1. Specifically:</p> 
        <ul>
          <li>
            Batch Size: 16, 32, 64, 128
          </li>
          <li>
            Learning Rate: 0.01, 0.0001
          </li>
          <li>
            Activation: Relu, Tanh
          </li>
          <li>
            Layers: 3, 5
          </li>
        </ul>
        <p>
          Once again, the total amount of models would have been 3 (data sets) x (4 (batch sizes) + 2 (learning rates) + 2 (activations) + 2 (layers)) = 24 models. However, to avoid overlap, and a repeating model with parameters batch size of 16, learning rate of 0.01, activation of Relu, and 3 layers, we omitted creating some models and the resulting amount was 21.
        </p>
        <h3>
            UNets Setup:
        </h3>
        <p>
            One of the models we implemented for deblurring our Gaussian blurred dataset in UNet. Unet consists of convolutional neural networks
            with 23 layers that focus on reconstructing an image based on a vector that is converted from an image's feature map. UNet also
            makes use of the feature map that is used to create the image vector to convert it back to an image.

            The Architecture of UNet consists of a contracting and expansive path. The first section consists of 3x3 convolution layers that are unpadded, ReLU
            activation functions and 2x2 max pooling layers. It has a downsampling stride of 2 where we double the feature channels each time. The expansive
            path upsamples the feature map upsampling with a 2x2 convolution for each step, that inversely, halves the number of feature channels,
            is added to the respective feature map, and 3x3 convolutions, each followed by a ReLU.
            Finally, the last layer is a 1x1 convolution used for mapping the feature vector.
        </p>
        <h3> Evaluation Metric</h3>
        <p>
          <p>
            We implemented both Peak Signal-to-Noise Ratio (PSNR) and Learned Perceptual Image Patch Similarity (LPIPS) metrics for image result comparison. PSNR is a ratio between the maximum possible value of a signal and the power of distorting noise that affects the quality of representation and LPIPS is used to judge the perceptual similarity between two images. We compared the original, clear image against the resulting image from the last epoch for similarity. 
          </p>
        </p>
      </section>
      <section class="box-results">
        <h2> Results </h2>
      </section>
      <div class="docs-first">
        <h3>
          Resulting Images for 3 Best and 3 Worst Models Across Phase 1 and Phase 2: <a href="https://docs.google.com/document/d/1Q8Re5uuez-BOK2VtmhQAcP-2S-zaX9xOa6TB4QTm1ec/edit?usp=sharing">access here</a>
        </h3>
        <iframe style="width:100%; height:90%; overflow: auto; display:block" src="https://docs.google.com/document/d/e/2PACX-1vSEyKbN-qeS8_PjY-3KX5PBX7kp3M9qnl0P9K2fQWrykexR8s0XPY4ZgXc9Vq4xwGE2H-1qoKqgQKGe/pub?embedded=true"></iframe>
      </div>
      <div class="docs">
        <h3>
          MSE Loss Graphs for Phase 1: Defocused, Motion, and Gaussian-Blurred Images: <a href="https://docs.google.com/document/d/1TfUvT3sBsVVNFZ76u_Ythx79yoLde5qK_cVFdPbawEw/edit?usp=sharing">access here</a>
        </h3>
        <iframe style="width:100%; height:90%; overflow: auto; display:block" src="https://docs.google.com/document/d/e/2PACX-1vSq9vcMNRGYMGhV2rpFPrE_sEbSg1PMA5YqOEY50RmjDEafqP-mptQrjmyrb3KF2Vb2GlFtXRCCZ6Ne/pub?embedded=true"></iframe>
      </div>
      <div class="docs">
        <h3>
          MSE Loss Graphs for Phase 2: Gaussian 5, Gaussian 10, and Gaussian 25-Blurred Images: <a href="https://docs.google.com/document/d/1xLqRIAZjYsurFgiXRZOHjTbToSwx0urCPnw-JlQavzg/edit?usp=sharing">access here</a>
        </h3>
        <iframe style="width:100%; height: 90%; overflow: auto; display:block" src="https://docs.google.com/document/d/e/2PACX-1vQayxrZiyQTIxHtActlDVSl7SB8TFG8qoNGidiJpV_lRlKEZ5t0KCM7Rqhm0BmYNi9OBRZhGd4qUNcP/pub?embedded=true"></iframe>
      </div>
      <div class="docs">
        <h3>
          PSNR and LPIPS Results for Phase 1 and Phase 2: <a href="https://docs.google.com/spreadsheets/d/1xWvjZf55JzujkNQahe0aPDgCq2lUQF_oaifnsfYNvYM/edit?usp=sharing">access here</a>
        </h3>
        <iframe style="width:100%; height: 90%; overflow: auto; display:block" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vTULSItvebkxcqNwLS8ZNZzifpz-RXIQsPab6Z5RjdBkRhzfk3JF2r0yV2c3E2uxMM_AQbSHSLqX4I9/pubhtml?widget=true&amp;headers=false"></iframe>
      </div>
    </section>
    <section class="box">
      <h2>Analyses </h2>
      <h3>
        Analysis of Phase 1:
    </h3>
    <p>
        We noticed that the best few models (as measured by LPIPS and PSNR values) were all trained on images that had been gaussian-blurred. The models we had trained on defocused and motion-blurred images did not perform as well, which we think is because they had a more natural and thus more intense blur. Motion-blurred images are taken while the camera is in action which means that in order to deblur the image, the model would have to learn the direction of motion in which the camera was moving and reconstruct the image on that basis, which it seems like our deep-learning model was not able to achieve well. Defocused images also have a heavier blur than Gaussian and are slightly shifted and zoomed in, which would mean that our model would have to readjust the image and deblur several layers to successfully bring back some clarity and sharpness. 
    </p>
    <p>
        The Gaussian-blurred images were artificially blurred after the image was taken, which is why we believe they were easier for our model to deblur them and restore their original resolution. The 7 best LPIPS and PSNR values from Phase 1 were all from Gaussian models so to further understand the role of hyperparameters and different levels of blur, we chose to train our next phase of models solely on Gaussian-blurred images.
    </p>
    </section>
    </main>
    <section id=sources>
        <h2>References</h2>
          <ul id=sourceList>
            <li>Albluwi, F., Krylov, V. A., & Dahyot, R. (2018). Image Deblurring and super-resolution using deep convolutional neural networks. 2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP). https://doi.org/10.1109/mlsp.2018.8516983 </li>
            <li>Ahmetoğlu, A. (2020, April 3). Pytorch implementation of VGG perceptual loss. GithubGist. Retrieved June 7, 2022, from https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49</li>
            <li>Aittala, M., & Durand, F. (2018). Burst image Deblurring using permutation invariant convolutional neural networks. Computer Vision – ECCV 2018, 748–764. https://doi.org/10.1007/978-3-030-01237-3_45</li>
            <li>Albluwi, F., Krylov, V. A., & Dahyot, R. (2018). Image Deblurring and super-resolution using deep convolutional neural networks. 2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP). https://doi.org/10.1109/mlsp.2018.8516983</li>
            <li>Alekseev, A. (2019, July 29). Blur Dataset. Kaggle. Retrieved June 7, 2022, from https://www.kaggle.com/datasets/kwentar/blur-dataset</li>
            <li>Lenka, M. K., & Mittal, A. (n.d.). Blind deblurring using Gans. Summer Research Fellowship Programme of India's Science Academies. Retrieved June 7, 2022, from http://reports.ias.ac.in/report/20694/blind-deblurring-using-gans</li>
            <li>Rajan, K., Zielesny, A., & Steinbeck, C. (2021). DECIMER 1.0: Deep Learning for chemical image recognition using Transformers. https://doi.org/10.26434/chemrxiv.14479287</li>
            <li>Rath, S. R. (2020, August 20). Image Deblurring using convolutional neural networks and Deep Learning. DebuggerCafe. Retrieved June 7, 2022, from https://debuggercafe.com/image-deblurring-using-convolutional-neural-networks-and-deep-learning/</li>
            <li>Remez, T., Litany, O., Giryes, R., & Bronstein, A. M. (2018). Class-aware fully convolutional gaussian and Poisson denoising. IEEE Transactions on Image Processing, 27(11), 5707–5722. https://doi.org/10.1109/tip.2018.2859044</li>
            <li>Zakirov, J., Ilin, E., & Zheltonozhskii, E. (2022, March 17). Bonlime/pytorch-tools: Tool box for pytorch. GitHub. Retrieved June 7, 2022, from https://github.com/bonlime/pytorch-tools</li>
            <li>Zhang, R. (2021, October 1). Perceptual Similarity: LPIPS metric. GitHub. Retrieved June 7, 2022, from https://github.com/richzhang/PerceptualSimilarity</li>
            <li>Zhu, J.-Y. (2020, June 27). Pytorch cyclegan and pix2pix. GitHub. Retrieved June 7, 2022, from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py </li>
          </ul>
    </section>
    <footer>
      <p>
        This page is for &copy;2022 CSE 455.
        All other attributions cited in page source.
      </p>
      <a href="https://github.com/EmiliaBorisova/imposter_syndrome">
        <img src="imgs/gh.png" alt="github repo" style="width:50px;height:50px;">
      </a>
    </footer>
  </body>
</html>


